{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec('from __future__ import absolute_import, division, print_function, unicode_literals')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('aps_failure_training_set_processed_8bit.csv')\n",
    "test = pd.read_csv('aps_failure_test_set_processed_8bit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test.copy()\n",
    "\n",
    "train_copy = train.copy()\n",
    "train_set = train_copy.sample(frac=0.9, random_state = 0)\n",
    "val_set = train_copy.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3048</td>\n",
       "      <td>-0.992188</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>-0.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19563</td>\n",
       "      <td>-0.992188</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-0.195312</td>\n",
       "      <td>-0.304688</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58303</td>\n",
       "      <td>-0.992188</td>\n",
       "      <td>-0.398438</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>-0.468750</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>-0.382812</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>-0.343750</td>\n",
       "      <td>-0.304688</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-0.304688</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>-0.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8870</td>\n",
       "      <td>-0.992188</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>-0.468750</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>0.460938</td>\n",
       "      <td>-0.257812</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>-0.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40228</td>\n",
       "      <td>-0.992188</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.468750</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445312</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>-0.203125</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>-0.023438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          class    aa_000    ab_000    ac_000    ad_000    ae_000    af_000  \\\n",
       "3048  -0.992188  0.015625 -0.289062  0.992188 -0.007812 -0.046875 -0.054688   \n",
       "19563 -0.992188  0.992188 -0.289062  0.992188 -0.007812 -0.046875 -0.054688   \n",
       "58303 -0.992188 -0.398438  0.554688 -0.468750 -0.007812 -0.046875 -0.054688   \n",
       "8870  -0.992188  0.992188  0.640625 -0.468750 -0.007812 -0.046875 -0.054688   \n",
       "40228 -0.992188  0.015625 -0.289062 -0.468750 -0.007812 -0.046875 -0.054688   \n",
       "\n",
       "         ag_000   ag_001    ag_002  ...    ee_002    ee_003    ee_004  \\\n",
       "3048  -0.007812 -0.03125 -0.054688  ... -0.015625  0.015625 -0.000000   \n",
       "19563 -0.007812 -0.03125 -0.054688  ...  0.992188  0.992188  0.992188   \n",
       "58303 -0.007812 -0.03125 -0.054688  ... -0.375000 -0.382812 -0.375000   \n",
       "8870  -0.007812 -0.03125 -0.046875  ...  0.992188  0.992188  0.992188   \n",
       "40228 -0.007812 -0.03125 -0.054688  ...  0.445312  0.453125  0.226562   \n",
       "\n",
       "         ee_005    ee_006    ee_007    ee_008    ee_009    ef_000    eg_000  \n",
       "3048   0.031250  0.117188 -0.031250  0.414062  0.085938 -0.023438 -0.023438  \n",
       "19563  0.992188 -0.187500 -0.195312 -0.304688 -0.171875  0.406250  0.375000  \n",
       "58303 -0.343750 -0.304688 -0.187500 -0.304688 -0.171875 -0.023438 -0.023438  \n",
       "8870   0.539062  0.289062  0.460938 -0.257812 -0.171875 -0.023438 -0.023438  \n",
       "40228  0.007812 -0.132812 -0.148438 -0.203125 -0.171875 -0.023438 -0.023438  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['class'] = (train_set['class'] > 0).astype(int)\n",
    "val_set['class'] = (val_set['class'] > 0).astype(int)\n",
    "test_set['class'] = (test_set['class'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 53092, 1: 908})\n",
      "Resampled dataset shape Counter({0: 53092, 1: 53092})\n",
      "Original dataset shape Counter({0: 5908, 1: 92})\n",
      "Resampled dataset shape Counter({0: 5908, 1: 5908})\n"
     ]
    }
   ],
   "source": [
    "# SMOTE TO Balance Dataset\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "\n",
    "print('Original dataset shape %s' % Counter(train_set['class'].to_numpy()))\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "X_res, y_res = sm.fit_resample(train_set.to_numpy(), train_set['class'].to_numpy())\n",
    "print('Resampled dataset shape %s' % Counter(y_res))\n",
    "\n",
    "print('Original dataset shape %s' % Counter(val_set['class'].to_numpy()))\n",
    "sm = SMOTE(random_state=42)\n",
    "X_val_res, y_val_res = sm.fit_resample(val_set.to_numpy(), val_set['class'].to_numpy())\n",
    "print('Resampled dataset shape %s' % Counter(y_val_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_pd = pd.Series(y_res)\n",
    "y_train = pd.get_dummies(y_res_pd)\n",
    "\n",
    "y_val_res_pd = pd.Series(y_val_res)\n",
    "y_val = pd.get_dummies(y_val_res_pd)\n",
    "\n",
    "train_set = pd.DataFrame(X_res, columns=list(train_set.columns.values))\n",
    "val_set = pd.DataFrame(X_val_res, columns=list(val_set.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.get_dummies(test_set['class'])\n",
    "\n",
    "\n",
    "\n",
    "train_set.drop(columns=['class'],inplace=True)\n",
    "val_set.drop(columns=['class'],inplace=True)\n",
    "test_set.drop(columns=['class'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>106184.000000</td>\n",
       "      <td>106184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.500002</td>\n",
       "      <td>0.500002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1\n",
       "count  106184.000000  106184.000000\n",
       "mean        0.500000       0.500000\n",
       "std         0.500002       0.500002\n",
       "min         0.000000       0.000000\n",
       "25%         0.000000       0.000000\n",
       "50%         0.500000       0.500000\n",
       "75%         1.000000       1.000000\n",
       "max         1.000000       1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_np = train_set.to_numpy()\n",
    "y_train_np = y_train.to_numpy().astype('float32')\n",
    "val_set_np = val_set.to_numpy()\n",
    "y_val_np = y_val.to_numpy().astype('float32')\n",
    "test_set_np = test_set.to_numpy()\n",
    "y_test_np = y_test.to_numpy().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN1D \n",
    "train_set_cnn = train_set_np.reshape(106184, 170, 1)\n",
    "val_set_cnn = val_set_np.reshape(11816, 170, 1)\n",
    "test_set_cnn = test_set_np.reshape(16000, 170, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss function\n",
    "\n",
    "class FocalLoss(keras.losses.Loss):\n",
    "    def __init__(self, gamma=5., alpha=.75,\n",
    "                 reduction=keras.losses.Reduction.AUTO, name='focal_loss'):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__(reduction=reduction,\n",
    "                                        name=name)\n",
    "        self.gamma = float(gamma)\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(\n",
    "            tf.subtract(1., model_out), self.gamma))\n",
    "        fl = tf.multiply(self.alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models (Focal Loss vs Cross Entropy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "###############################################################################################################\n",
    "# *************************************************************************************************************\n",
    "###############################################################################################################\n",
    "\n",
    "model_cnn_fl = tf.keras.Sequential()\n",
    "model_cnn_fl.add(layers.Conv1D(10, 1, activation='relu', input_shape=(170, 1),data_format='channels_last'))\n",
    "model_cnn_fl.add(layers.Flatten())\n",
    "model_cnn_fl.add(layers.Dense(64, activation='relu', bias_initializer=tf.constant_initializer(0.01),\n",
    "                              kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model_cnn_fl.add(layers.Dropout(0.5))\n",
    "model_cnn_fl.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model_cnn_fl.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss=FocalLoss(),\n",
    "              metrics=METRICS)\n",
    "\n",
    "###############################################################################################################\n",
    "# *************************************************************************************************************\n",
    "###############################################################################################################\n",
    "\n",
    "model_cnn_bc = tf.keras.Sequential()\n",
    "model_cnn_bc.add(layers.Conv1D(10, 1, activation='relu', input_shape=(170, 1),data_format='channels_last'))\n",
    "model_cnn_bc.add(layers.Flatten())\n",
    "model_cnn_bc.add(layers.Dense(64, activation='relu', bias_initializer=tf.constant_initializer(0.01),\n",
    "                              kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model_cnn_bc.add(layers.Dropout(0.5))\n",
    "model_cnn_bc.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model_cnn_bc.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106184 samples, validate on 11816 samples\n",
      "Epoch 1/20\n",
      "106184/106184 [==============================] - 7s 70us/sample - loss: 0.0124 - tp: 99931.0000 - fp: 6253.0000 - tn: 99931.0000 - fn: 6253.0000 - accuracy: 0.9411 - precision: 0.9411 - recall: 0.9411 - auc: 0.9773 - val_loss: 0.0046 - val_tp: 11427.0000 - val_fp: 389.0000 - val_tn: 11427.0000 - val_fn: 389.0000 - val_accuracy: 0.9671 - val_precision: 0.9671 - val_recall: 0.9671 - val_auc: 0.9949\n",
      "Epoch 2/20\n",
      "106184/106184 [==============================] - 5s 47us/sample - loss: 0.0059 - tp: 100325.0000 - fp: 5859.0000 - tn: 100325.0000 - fn: 5859.0000 - accuracy: 0.9448 - precision: 0.9448 - recall: 0.9448 - auc: 0.9837 - val_loss: 0.0047 - val_tp: 11437.0000 - val_fp: 379.0000 - val_tn: 11437.0000 - val_fn: 379.0000 - val_accuracy: 0.9679 - val_precision: 0.9679 - val_recall: 0.9679 - val_auc: 0.9951\n",
      "Epoch 3/20\n",
      "106184/106184 [==============================] - 6s 52us/sample - loss: 0.0064 - tp: 99931.0000 - fp: 6253.0000 - tn: 99931.0000 - fn: 6253.0000 - accuracy: 0.9411 - precision: 0.9411 - recall: 0.9411 - auc: 0.9813 - val_loss: 0.0042 - val_tp: 11484.0000 - val_fp: 332.0000 - val_tn: 11484.0000 - val_fn: 332.0000 - val_accuracy: 0.9719 - val_precision: 0.9719 - val_recall: 0.9719 - val_auc: 0.9960\n",
      "Epoch 4/20\n",
      "106184/106184 [==============================] - 6s 53us/sample - loss: 0.0061 - tp: 99982.0000 - fp: 6202.0000 - tn: 99982.0000 - fn: 6202.0000 - accuracy: 0.9416 - precision: 0.9416 - recall: 0.9416 - auc: 0.9813 - val_loss: 0.0044 - val_tp: 11445.0000 - val_fp: 371.0000 - val_tn: 11445.0000 - val_fn: 371.0000 - val_accuracy: 0.9686 - val_precision: 0.9686 - val_recall: 0.9686 - val_auc: 0.9945\n",
      "Epoch 5/20\n",
      "106184/106184 [==============================] - 6s 54us/sample - loss: 0.0059 - tp: 100033.0000 - fp: 6151.0000 - tn: 100033.0000 - fn: 6151.0000 - accuracy: 0.9421 - precision: 0.9421 - recall: 0.9421 - auc: 0.9813 - val_loss: 0.0038 - val_tp: 11454.0000 - val_fp: 362.0000 - val_tn: 11454.0000 - val_fn: 362.0000 - val_accuracy: 0.9694 - val_precision: 0.9694 - val_recall: 0.9694 - val_auc: 0.9946\n",
      "Epoch 6/20\n",
      "106184/106184 [==============================] - 4s 42us/sample - loss: 0.0062 - tp: 99804.0000 - fp: 6380.0000 - tn: 99804.0000 - fn: 6380.0000 - accuracy: 0.9399 - precision: 0.9399 - recall: 0.9399 - auc: 0.9795 - val_loss: 0.0038 - val_tp: 11472.0000 - val_fp: 344.0000 - val_tn: 11472.0000 - val_fn: 344.0000 - val_accuracy: 0.9709 - val_precision: 0.9709 - val_recall: 0.9709 - val_auc: 0.9943\n",
      "Epoch 7/20\n",
      "106184/106184 [==============================] - 5s 45us/sample - loss: 0.0061 - tp: 99760.0000 - fp: 6424.0000 - tn: 99760.0000 - fn: 6424.0000 - accuracy: 0.9395 - precision: 0.9395 - recall: 0.9395 - auc: 0.9795 - val_loss: 0.0044 - val_tp: 11337.0000 - val_fp: 479.0000 - val_tn: 11337.0000 - val_fn: 479.0000 - val_accuracy: 0.9595 - val_precision: 0.9595 - val_recall: 0.9595 - val_auc: 0.9916\n",
      "Epoch 8/20\n",
      "106184/106184 [==============================] - 5s 44us/sample - loss: 0.0062 - tp: 99689.0000 - fp: 6495.0000 - tn: 99689.0000 - fn: 6495.0000 - accuracy: 0.9388 - precision: 0.9388 - recall: 0.9388 - auc: 0.9784 - val_loss: 0.0040 - val_tp: 11372.0000 - val_fp: 444.0000 - val_tn: 11372.0000 - val_fn: 444.0000 - val_accuracy: 0.9624 - val_precision: 0.9624 - val_recall: 0.9624 - val_auc: 0.9935\n",
      "Epoch 9/20\n",
      "106184/106184 [==============================] - 4s 42us/sample - loss: 0.0063 - tp: 99619.0000 - fp: 6565.0000 - tn: 99619.0000 - fn: 6565.0000 - accuracy: 0.9382 - precision: 0.9382 - recall: 0.9382 - auc: 0.9759 - val_loss: 0.0047 - val_tp: 11388.0000 - val_fp: 428.0000 - val_tn: 11388.0000 - val_fn: 428.0000 - val_accuracy: 0.9638 - val_precision: 0.9638 - val_recall: 0.9638 - val_auc: 0.9889\n",
      "Epoch 10/20\n",
      "106184/106184 [==============================] - 5s 44us/sample - loss: 0.0063 - tp: 99583.0000 - fp: 6601.0000 - tn: 99583.0000 - fn: 6601.0000 - accuracy: 0.9378 - precision: 0.9378 - recall: 0.9378 - auc: 0.9755 - val_loss: 0.0048 - val_tp: 11388.0000 - val_fp: 428.0000 - val_tn: 11388.0000 - val_fn: 428.0000 - val_accuracy: 0.9638 - val_precision: 0.9638 - val_recall: 0.9638 - val_auc: 0.9936\n",
      "Epoch 11/20\n",
      "106184/106184 [==============================] - 6s 53us/sample - loss: 0.0065 - tp: 99562.0000 - fp: 6622.0000 - tn: 99562.0000 - fn: 6622.0000 - accuracy: 0.9376 - precision: 0.9376 - recall: 0.9376 - auc: 0.9752 - val_loss: 0.0051 - val_tp: 11345.0000 - val_fp: 471.0000 - val_tn: 11345.0000 - val_fn: 471.0000 - val_accuracy: 0.9601 - val_precision: 0.9601 - val_recall: 0.9601 - val_auc: 0.9876\n",
      "Epoch 12/20\n",
      "106184/106184 [==============================] - 5s 47us/sample - loss: 0.0065 - tp: 99470.0000 - fp: 6714.0000 - tn: 99470.0000 - fn: 6714.0000 - accuracy: 0.9368 - precision: 0.9368 - recall: 0.9368 - auc: 0.9745 - val_loss: 0.0048 - val_tp: 11347.0000 - val_fp: 469.0000 - val_tn: 11347.0000 - val_fn: 469.0000 - val_accuracy: 0.9603 - val_precision: 0.9603 - val_recall: 0.9603 - val_auc: 0.9933\n",
      "Epoch 13/20\n",
      "106184/106184 [==============================] - 5s 43us/sample - loss: 0.0067 - tp: 99339.0000 - fp: 6845.0000 - tn: 99339.0000 - fn: 6845.0000 - accuracy: 0.9355 - precision: 0.9355 - recall: 0.9355 - auc: 0.9730 - val_loss: 0.0060 - val_tp: 11129.0000 - val_fp: 687.0000 - val_tn: 11129.0000 - val_fn: 687.0000 - val_accuracy: 0.9419 - val_precision: 0.9419 - val_recall: 0.9419 - val_auc: 0.9833\n",
      "Epoch 14/20\n",
      "106184/106184 [==============================] - 4s 38us/sample - loss: 0.0064 - tp: 99651.0000 - fp: 6533.0000 - tn: 99651.0000 - fn: 6533.0000 - accuracy: 0.9385 - precision: 0.9385 - recall: 0.9385 - auc: 0.9754 - val_loss: 0.0046 - val_tp: 11397.0000 - val_fp: 419.0000 - val_tn: 11397.0000 - val_fn: 419.0000 - val_accuracy: 0.9645 - val_precision: 0.9645 - val_recall: 0.9645 - val_auc: 0.9898\n",
      "Epoch 15/20\n",
      "106184/106184 [==============================] - 4s 38us/sample - loss: 0.0063 - tp: 99680.0000 - fp: 6504.0000 - tn: 99680.0000 - fn: 6504.0000 - accuracy: 0.9387 - precision: 0.9387 - recall: 0.9387 - auc: 0.9762 - val_loss: 0.0047 - val_tp: 11406.0000 - val_fp: 410.0000 - val_tn: 11406.0000 - val_fn: 410.0000 - val_accuracy: 0.9653 - val_precision: 0.9653 - val_recall: 0.9653 - val_auc: 0.9920\n",
      "Epoch 16/20\n",
      "106184/106184 [==============================] - 4s 38us/sample - loss: 0.0066 - tp: 99516.0000 - fp: 6668.0000 - tn: 99516.0000 - fn: 6668.0000 - accuracy: 0.9372 - precision: 0.9372 - recall: 0.9372 - auc: 0.9743 - val_loss: 0.0042 - val_tp: 11462.0000 - val_fp: 354.0000 - val_tn: 11462.0000 - val_fn: 354.0000 - val_accuracy: 0.9700 - val_precision: 0.9700 - val_recall: 0.9700 - val_auc: 0.9937\n",
      "Epoch 17/20\n",
      "106184/106184 [==============================] - 4s 36us/sample - loss: 0.0065 - tp: 99520.0000 - fp: 6664.0000 - tn: 99520.0000 - fn: 6664.0000 - accuracy: 0.9372 - precision: 0.9372 - recall: 0.9372 - auc: 0.9752 - val_loss: 0.0045 - val_tp: 11417.0000 - val_fp: 399.0000 - val_tn: 11417.0000 - val_fn: 399.0000 - val_accuracy: 0.9662 - val_precision: 0.9662 - val_recall: 0.9662 - val_auc: 0.9896\n",
      "Epoch 18/20\n",
      "106184/106184 [==============================] - 4s 38us/sample - loss: 0.0065 - tp: 99604.0000 - fp: 6580.0000 - tn: 99604.0000 - fn: 6580.0000 - accuracy: 0.9380 - precision: 0.9380 - recall: 0.9380 - auc: 0.9745 - val_loss: 0.0044 - val_tp: 11382.0000 - val_fp: 434.0000 - val_tn: 11382.0000 - val_fn: 434.0000 - val_accuracy: 0.9633 - val_precision: 0.9633 - val_recall: 0.9633 - val_auc: 0.9922\n",
      "Epoch 19/20\n",
      "106184/106184 [==============================] - 4s 39us/sample - loss: 0.0066 - tp: 99534.0000 - fp: 6650.0000 - tn: 99534.0000 - fn: 6650.0000 - accuracy: 0.9374 - precision: 0.9374 - recall: 0.9374 - auc: 0.9744 - val_loss: 0.0047 - val_tp: 11450.0000 - val_fp: 366.0000 - val_tn: 11450.0000 - val_fn: 366.0000 - val_accuracy: 0.9690 - val_precision: 0.9690 - val_recall: 0.9690 - val_auc: 0.9917\n",
      "Epoch 20/20\n",
      "106184/106184 [==============================] - 4s 35us/sample - loss: 0.0066 - tp: 99537.0000 - fp: 6647.0000 - tn: 99537.0000 - fn: 6647.0000 - accuracy: 0.9374 - precision: 0.9374 - recall: 0.9374 - auc: 0.9737 - val_loss: 0.0053 - val_tp: 11278.0000 - val_fp: 538.0000 - val_tn: 11278.0000 - val_fn: 538.0000 - val_accuracy: 0.9545 - val_precision: 0.9545 - val_recall: 0.9545 - val_auc: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63c831c50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_fl.fit(train_set_cnn,y_train_np, batch_size=256, epochs=20, validation_data=(val_set_cnn,y_val_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106184 samples, validate on 11816 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "106184/106184 [==============================] - 7s 70us/sample - loss: 0.1554 - tp: 101638.0000 - fp: 4546.0000 - tn: 101638.0000 - fn: 4546.0000 - accuracy: 0.9572 - precision: 0.9572 - recall: 0.9572 - auc: 0.9880 - val_loss: 0.1193 - val_tp: 11475.0000 - val_fp: 341.0000 - val_tn: 11475.0000 - val_fn: 341.0000 - val_accuracy: 0.9711 - val_precision: 0.9711 - val_recall: 0.9711 - val_auc: 0.9932\n",
      "Epoch 2/20\n",
      "106184/106184 [==============================] - 6s 54us/sample - loss: 0.1313 - tp: 102404.0000 - fp: 3780.0000 - tn: 102404.0000 - fn: 3780.0000 - accuracy: 0.9644 - precision: 0.9644 - recall: 0.9644 - auc: 0.9922 - val_loss: 0.0979 - val_tp: 11535.0000 - val_fp: 281.0000 - val_tn: 11535.0000 - val_fn: 281.0000 - val_accuracy: 0.9762 - val_precision: 0.9762 - val_recall: 0.9762 - val_auc: 0.9961\n",
      "Epoch 3/20\n",
      "106184/106184 [==============================] - 4s 39us/sample - loss: 0.1255 - tp: 102656.0000 - fp: 3528.0000 - tn: 102656.0000 - fn: 3528.0000 - accuracy: 0.9668 - precision: 0.9668 - recall: 0.9668 - auc: 0.9931 - val_loss: 0.0958 - val_tp: 11523.0000 - val_fp: 293.0000 - val_tn: 11523.0000 - val_fn: 293.0000 - val_accuracy: 0.9752 - val_precision: 0.9752 - val_recall: 0.9752 - val_auc: 0.9961\n",
      "Epoch 4/20\n",
      "106184/106184 [==============================] - 4s 36us/sample - loss: 0.1214 - tp: 102730.0000 - fp: 3454.0000 - tn: 102730.0000 - fn: 3454.0000 - accuracy: 0.9675 - precision: 0.9675 - recall: 0.9675 - auc: 0.9935 - val_loss: 0.0967 - val_tp: 11525.0000 - val_fp: 291.0000 - val_tn: 11525.0000 - val_fn: 291.0000 - val_accuracy: 0.9754 - val_precision: 0.9754 - val_recall: 0.9754 - val_auc: 0.9970\n",
      "Epoch 5/20\n",
      "106184/106184 [==============================] - 4s 39us/sample - loss: 0.1187 - tp: 102969.0000 - fp: 3215.0000 - tn: 102969.0000 - fn: 3215.0000 - accuracy: 0.9697 - precision: 0.9697 - recall: 0.9697 - auc: 0.9938 - val_loss: 0.0930 - val_tp: 11542.0000 - val_fp: 274.0000 - val_tn: 11542.0000 - val_fn: 274.0000 - val_accuracy: 0.9768 - val_precision: 0.9768 - val_recall: 0.9768 - val_auc: 0.9970\n",
      "Epoch 6/20\n",
      "106184/106184 [==============================] - 4s 39us/sample - loss: 0.1180 - tp: 103100.0000 - fp: 3084.0000 - tn: 103100.0000 - fn: 3084.0000 - accuracy: 0.9710 - precision: 0.9710 - recall: 0.9710 - auc: 0.9939 - val_loss: 0.1028 - val_tp: 11492.0000 - val_fp: 324.0000 - val_tn: 11492.0000 - val_fn: 324.0000 - val_accuracy: 0.9726 - val_precision: 0.9726 - val_recall: 0.9726 - val_auc: 0.9963\n",
      "Epoch 7/20\n",
      "106184/106184 [==============================] - 4s 36us/sample - loss: 0.1173 - tp: 103131.0000 - fp: 3053.0000 - tn: 103131.0000 - fn: 3053.0000 - accuracy: 0.9712 - precision: 0.9712 - recall: 0.9712 - auc: 0.9940 - val_loss: 0.0893 - val_tp: 11553.0000 - val_fp: 263.0000 - val_tn: 11553.0000 - val_fn: 263.0000 - val_accuracy: 0.9777 - val_precision: 0.9777 - val_recall: 0.9777 - val_auc: 0.9973\n",
      "Epoch 8/20\n",
      "106184/106184 [==============================] - 4s 42us/sample - loss: 0.1159 - tp: 103162.0000 - fp: 3022.0000 - tn: 103162.0000 - fn: 3022.0000 - accuracy: 0.9715 - precision: 0.9715 - recall: 0.9715 - auc: 0.9941 - val_loss: 0.1105 - val_tp: 11502.0000 - val_fp: 314.0000 - val_tn: 11502.0000 - val_fn: 314.0000 - val_accuracy: 0.9734 - val_precision: 0.9734 - val_recall: 0.9734 - val_auc: 0.9943\n",
      "Epoch 9/20\n",
      "106184/106184 [==============================] - 5s 48us/sample - loss: 0.1175 - tp: 103099.0000 - fp: 3085.0000 - tn: 103099.0000 - fn: 3085.0000 - accuracy: 0.9709 - precision: 0.9709 - recall: 0.9709 - auc: 0.9941 - val_loss: 0.1005 - val_tp: 11531.0000 - val_fp: 285.0000 - val_tn: 11531.0000 - val_fn: 285.0000 - val_accuracy: 0.9759 - val_precision: 0.9759 - val_recall: 0.9759 - val_auc: 0.9958\n",
      "Epoch 10/20\n",
      "106184/106184 [==============================] - 6s 54us/sample - loss: 0.1145 - tp: 103122.0000 - fp: 3062.0000 - tn: 103122.0000 - fn: 3062.0000 - accuracy: 0.9712 - precision: 0.9712 - recall: 0.9712 - auc: 0.9944 - val_loss: 0.0993 - val_tp: 11529.0000 - val_fp: 287.0000 - val_tn: 11529.0000 - val_fn: 287.0000 - val_accuracy: 0.9757 - val_precision: 0.9757 - val_recall: 0.9757 - val_auc: 0.9962\n",
      "Epoch 11/20\n",
      "106184/106184 [==============================] - 7s 63us/sample - loss: 0.1108 - tp: 103382.0000 - fp: 2802.0000 - tn: 103382.0000 - fn: 2802.0000 - accuracy: 0.9736 - precision: 0.9736 - recall: 0.9736 - auc: 0.9946 - val_loss: 0.0942 - val_tp: 11540.0000 - val_fp: 276.0000 - val_tn: 11540.0000 - val_fn: 276.0000 - val_accuracy: 0.9766 - val_precision: 0.9766 - val_recall: 0.9766 - val_auc: 0.9971\n",
      "Epoch 12/20\n",
      "106184/106184 [==============================] - 6s 55us/sample - loss: 0.1139 - tp: 103284.0000 - fp: 2900.0000 - tn: 103284.0000 - fn: 2900.0000 - accuracy: 0.9727 - precision: 0.9727 - recall: 0.9727 - auc: 0.9944 - val_loss: 0.0941 - val_tp: 11530.0000 - val_fp: 286.0000 - val_tn: 11530.0000 - val_fn: 286.0000 - val_accuracy: 0.9758 - val_precision: 0.9758 - val_recall: 0.9758 - val_auc: 0.9972\n",
      "Epoch 13/20\n",
      "106184/106184 [==============================] - 6s 53us/sample - loss: 0.1162 - tp: 103271.0000 - fp: 2913.0000 - tn: 103271.0000 - fn: 2913.0000 - accuracy: 0.9726 - precision: 0.9726 - recall: 0.9726 - auc: 0.9941 - val_loss: 0.1020 - val_tp: 11518.0000 - val_fp: 298.0000 - val_tn: 11518.0000 - val_fn: 298.0000 - val_accuracy: 0.9748 - val_precision: 0.9748 - val_recall: 0.9748 - val_auc: 0.9968\n",
      "Epoch 14/20\n",
      "106184/106184 [==============================] - 5s 48us/sample - loss: 0.1185 - tp: 103188.0000 - fp: 2996.0000 - tn: 103188.0000 - fn: 2996.0000 - accuracy: 0.9718 - precision: 0.9718 - recall: 0.9718 - auc: 0.9938 - val_loss: 0.0878 - val_tp: 11570.0000 - val_fp: 246.0000 - val_tn: 11570.0000 - val_fn: 246.0000 - val_accuracy: 0.9792 - val_precision: 0.9792 - val_recall: 0.9792 - val_auc: 0.9979\n",
      "Epoch 15/20\n",
      "106184/106184 [==============================] - 6s 57us/sample - loss: 0.1107 - tp: 103451.0000 - fp: 2733.0000 - tn: 103451.0000 - fn: 2733.0000 - accuracy: 0.9743 - precision: 0.9743 - recall: 0.9743 - auc: 0.9946 - val_loss: 0.1024 - val_tp: 11509.0000 - val_fp: 307.0000 - val_tn: 11509.0000 - val_fn: 307.0000 - val_accuracy: 0.9740 - val_precision: 0.9740 - val_recall: 0.9740 - val_auc: 0.9962\n",
      "Epoch 16/20\n",
      "106184/106184 [==============================] - 5s 45us/sample - loss: 0.1178 - tp: 103243.0000 - fp: 2941.0000 - tn: 103243.0000 - fn: 2941.0000 - accuracy: 0.9723 - precision: 0.9723 - recall: 0.9723 - auc: 0.9937 - val_loss: 0.0963 - val_tp: 11543.0000 - val_fp: 273.0000 - val_tn: 11543.0000 - val_fn: 273.0000 - val_accuracy: 0.9769 - val_precision: 0.9769 - val_recall: 0.9769 - val_auc: 0.9968\n",
      "Epoch 17/20\n",
      "106184/106184 [==============================] - 6s 57us/sample - loss: 0.1138 - tp: 103382.0000 - fp: 2802.0000 - tn: 103382.0000 - fn: 2802.0000 - accuracy: 0.9736 - precision: 0.9736 - recall: 0.9736 - auc: 0.9938 - val_loss: 0.0904 - val_tp: 11579.0000 - val_fp: 237.0000 - val_tn: 11579.0000 - val_fn: 237.0000 - val_accuracy: 0.9799 - val_precision: 0.9799 - val_recall: 0.9799 - val_auc: 0.9975\n",
      "Epoch 18/20\n",
      "106184/106184 [==============================] - 5s 47us/sample - loss: 0.1192 - tp: 103208.0000 - fp: 2976.0000 - tn: 103208.0000 - fn: 2976.0000 - accuracy: 0.9720 - precision: 0.9720 - recall: 0.9720 - auc: 0.9935 - val_loss: 0.1011 - val_tp: 11567.0000 - val_fp: 249.0000 - val_tn: 11567.0000 - val_fn: 249.0000 - val_accuracy: 0.9789 - val_precision: 0.9789 - val_recall: 0.9789 - val_auc: 0.9967\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106184/106184 [==============================] - 7s 63us/sample - loss: 0.1141 - tp: 103417.0000 - fp: 2767.0000 - tn: 103417.0000 - fn: 2767.0000 - accuracy: 0.9739 - precision: 0.9739 - recall: 0.9739 - auc: 0.9939 - val_loss: 0.0917 - val_tp: 11578.0000 - val_fp: 238.0000 - val_tn: 11578.0000 - val_fn: 238.0000 - val_accuracy: 0.9799 - val_precision: 0.9799 - val_recall: 0.9799 - val_auc: 0.9970\n",
      "Epoch 20/20\n",
      "106184/106184 [==============================] - 5s 45us/sample - loss: 0.1181 - tp: 103288.0000 - fp: 2896.0000 - tn: 103288.0000 - fn: 2896.0000 - accuracy: 0.9727 - precision: 0.9727 - recall: 0.9727 - auc: 0.9931 - val_loss: 0.0950 - val_tp: 11563.0000 - val_fp: 253.0000 - val_tn: 11563.0000 - val_fn: 253.0000 - val_accuracy: 0.9786 - val_precision: 0.9786 - val_recall: 0.9786 - val_auc: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x638031a58>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_bc.fit(train_set_cnn,y_train_np, batch_size=256, epochs=20, validation_data=(val_set_cnn,y_val_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cost(result, loss_name):\n",
    "    Cost_1 = 10\n",
    "    Cost_2 = 500\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_ori.to_numpy(), result.to_numpy()).ravel()\n",
    "    cost = Cost_1*fp + Cost_2*fn\n",
    "    print('tn: ',tn, 'fp: ',fp,'fn: ',fn,'tp: ',tp)\n",
    "    print('Final Cost using ',loss_name,' is: ',cost)\n",
    "    print('Total number of misclassificactions: ', fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL:  [[14311  1314]\n",
      " [    5   370]]\n",
      "BC:  [[15272   353]\n",
      " [   32   343]]\n",
      "tn:  14311 fp:  1314 fn:  5 tp:  370\n",
      "Final Cost using  Focal Loss - CNN1D  is:  15640\n",
      "Total number of misclassificactions:  1319\n",
      "tn:  15272 fp:  353 fn:  32 tp:  343\n",
      "Final Cost using  Binary Cross Entropy - CNN1D  is:  19530\n",
      "Total number of misclassificactions:  385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "result_cnn_fl = model_cnn_fl.predict(test_set_cnn)\n",
    "result_cnn_bc = model_cnn_bc.predict(test_set_cnn)\n",
    "# print('FL CNN1D Output Shape:', result_cnn_fl.shape)\n",
    "# print('BC CNN1D Output Shape:', result_cnn_bc.shape)\n",
    "\n",
    "assert result_cnn_fl.shape == result_cnn_bc.shape\n",
    "\n",
    "result_cnn_fl_pd = pd.DataFrame(np.rint(result_cnn_fl))\n",
    "result_cnn_fl_stack_pd = result_cnn_fl_pd.stack()\n",
    "result_cnn_fl_ori = pd.Series(pd.Categorical(result_cnn_fl_stack_pd[result_cnn_fl_stack_pd!=0].index.get_level_values(1)))\n",
    "\n",
    "result_cnn_bc_pd = pd.DataFrame(np.rint(result_cnn_bc))\n",
    "result_cnn_bc_stack_pd = result_cnn_bc_pd.stack()\n",
    "result_cnn_bc_ori = pd.Series(pd.Categorical(result_cnn_bc_stack_pd[result_cnn_bc_stack_pd!=0].index.get_level_values(1)))\n",
    "\n",
    "y_test_stack = y_test.stack()\n",
    "y_test_ori = pd.Series(pd.Categorical(y_test_stack[y_test_stack!=0].index.get_level_values(1)))\n",
    "\n",
    "print('FL: ',confusion_matrix(y_test_ori.to_numpy(),result_cnn_fl_ori.to_numpy()))\n",
    "print('BC: ',confusion_matrix(y_test_ori.to_numpy(),result_cnn_bc_ori.to_numpy()))\n",
    "\n",
    "final_cost(result_cnn_fl_ori, 'Focal Loss - CNN1D')\n",
    "final_cost(result_cnn_bc_ori, 'Binary Cross Entropy - CNN1D')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
